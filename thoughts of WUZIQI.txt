todolist：
1.build the map;
2.realize the choosing position
3.computer choose position
4.judge the winner.
5.computer choose better
details:
1.mouse choose? consele choose?
some information:
第一大脑: 落子选择器 
AlphaGo的第一个神经网络大脑是“监督学习的策略网络(Policy Network)” ，
观察棋盘布局企图找到最佳的下一步。
事实上，它预测每一个合法下一步的最佳概率，那么最前面猜测的就是那个概率最高的。
你可以理解成“落子选择器”。
AlphaGo系统事实上需要两个额外落子选择器的大脑。
一个是“强化学习的策略网络（Policy Network）”，通过百万级额外的模拟局来完成。
你可以称之为更强的。
比起基本的训练，只是教网络去模仿单一人类的落子，高级的训练会与每一个模拟棋局下到底，教网络最可能赢的下一手。
Sliver团队通过更强的落子选择器总结了百万级训练棋局，比他们之前版本又迭代了不少。
单单用这种落子选择器就已经是强大的对手了，可以到业余棋手的水平，或者说跟之前最强的围棋AI媲美。
这里重点是这种落子选择器不会去“读”。它就是简单审视从单一棋盘位置，再提出从那个位置分析出来的落子。
它不会去模拟任何未来的走法。这展示了简单的深度神经网络学习的力量。
第二大脑：棋局评估器 （Position Evaluator）
AlphaGo的第二个大脑相对于落子选择器是回答另一个问题。
不是去猜测具体下一步，它预测每一个棋手赢棋的可能，在给定棋子位置情况下。
这“局面评估器”就是论文中提到的“价值网络（Value Network)”，通过整体局面判断来辅助落子选择器。
这个判断仅仅是大概的，但对于阅读速度提高很有帮助。
通过分类潜在的未来局面的“好”与“坏”，AlphaGo能够决定是否通过特殊变种去深入阅读。
如果局面评估器说这个特殊变种不行，那么AI就跳过阅读在这一条线上的任何更多落子。
蒙特卡洛树搜索算法
如果拥有无限的计算能力，MCTS可以理论上去计算最佳落子通过探索每一局的可能步骤。
但未来走法的搜索空间对于围棋来说太大了（大到比我们认知宇宙里的粒子还多），实际上AI没有办法探索每一个可能的变种。
MCTS做法比其他AI有多好的原因是在识别有利的变种，这样可以跳过一些不利的。
Silver团队让AlphaGo装上MCTS系统的模块，这种框架让设计者去嵌入不同的功能去评估变种。
最后马力全开的AlphaGo系统按如下方式使用了所有这些大脑。
1. 从当前的棋盘布局，选择哪些下一步的可能性。
他们用基础的落子选择器大脑（他们尝试使用更强的版本，但事实上让AlphaGo更弱，因为这没有让MCTS提供更广阔的选择空间）。
它集中在“明显最好”的落子而不是阅读很多，而不是再去选择也许对后来有利的下法。
2. 对于每一个可能的落子，评估质量有两种方式：要么用棋盘上局面评估器在落子后，要么运行更深入蒙特卡罗模拟器（滚动）去思考未来的落子，使用快速阅读的落子选择器去提高搜索速度。
AlphaGo使用简单参数，“混合相关系数”，将每一个猜测取权重。
最大马力的AlphaGo使用 50/50的混合比，使用局面评估器和模拟化滚动去做平衡判断。